{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying *text* data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To this point, we haven't been classifying text data, really. The question is: If we are working with text, what should the features be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data set: Random acts of pizza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Kaggle link](https://www.kaggle.com/c/random-acts-of-pizza)\n",
    "\n",
    "As described on Kaggle:\n",
    "\n",
    "> This competition contains a dataset with 5671 textual requests for pizza from the Reddit community Random Acts of Pizza together with their outcome (successful/unsuccessful) and meta-data. Participants must create an algorithm capable of predicting which requests will garner a cheesy (but sincere!) act of kindness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we'll read it in, and take a look at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "csvfile = open('corpora/pizza.csv')\n",
    "pizza_reader = csv.reader(csvfile)\n",
    "data_list = []\n",
    "for row in pizza_reader:\n",
    "    data_list.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next we have to tokenize it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note: Tokenizing this turned out to be a little of a pain. \n",
    "The problem was that, since this was text posted online, people used all sorts of abbreviations, and contractions were important.\n",
    "\n",
    "I finally ended up using a special purposes tokenizer I created for some twitter data. This uses some techniques that will learn about in a later week. Basically, it's the result of a lot of fiddling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bruces_twitter_tokenizer(text):\n",
    "    import re\n",
    "    \n",
    "    def is_contraction(the_text):\n",
    "        contraction_patterns = re.compile(r\"(?i)(.)('ll|'re|'ve|n't|'s|'m|'d)\\b\")\n",
    "        return contraction_patterns.search(the_text)\n",
    "    \n",
    "    punctuation_class = r\"([\\.\\-\\/&\\\";:\\(\\)\\?\\!\\]\\[\\{\\}\\*#])\"\n",
    "    \n",
    "    # eliminate_urls\n",
    "    text = re.sub(r\"http\\S*\", \"\", text)\n",
    "    \n",
    "    # elimintate hashtags, user mentionds\n",
    "    text = re.sub(r\"#\\S*\", \"\", text)\n",
    "    text = re.sub(r\"@\\S*\", \"\", text)\n",
    "    \n",
    "    # Separate most punctuation at end of words\n",
    "\n",
    "    text = re.sub(r\"(\\w)\" + punctuation_class, r'\\1 \\2 ', text)\n",
    "    \n",
    "    # Separate most punctuation at start of words\n",
    "    text = re.sub(punctuation_class + r\"(\\w)\", r'\\1 \\2', text)\n",
    "    \n",
    "    # Separate punctuation from other punctuation\n",
    "    text = re.sub(punctuation_class + punctuation_class, r'\\1 \\2 ', text)\n",
    "    \n",
    "    # Put spaces between + and = signs and digits. Also %s that follow a digit, $s that come before a digit\n",
    "    text = re.sub(r\"(\\d)([+=%])\", r'\\1 \\2 ', text)\n",
    "    text = re.sub(r\"([\\$+=])(\\d)\", r'\\1 \\2', text)\n",
    "    \n",
    "    # Separate commas if they're followed by space.\n",
    "    # (E.g., don't separate 2,500)\n",
    "    text = re.sub(r\"(,\\s)\", r' \\1', text)\n",
    "    \n",
    "    #when we have two double quotes make it 1.\n",
    "    #\n",
    "    text = re.sub(\"\\\"\\\"\", \"\\\"\", text)\n",
    "\n",
    "    # Separate leading and trailing single and double quotes .\n",
    "    text = re.sub(r\"(\\'\\s)\", r' \\1', text)\n",
    "    text = re.sub(r\"(\\s\\')\", r'\\1 ', text)\n",
    "    text = re.sub(r\"(\\\"\\s)\", r' \\1', text)\n",
    "    text = re.sub(r\"(\\s\\\")\", r'\\1 ', text)\n",
    "    text = re.sub(r\"(^\\\")\", r'\\1 ', text)\n",
    "    text = re.sub(r\"(^\\')\", r'\\1 ', text)\n",
    "    text = re.sub(r\"('\\'$)\", r' \\1', text)\n",
    "    text = re.sub(r\"('\\\"$)\", r' \\1', text)\n",
    "\n",
    "    #Separate parentheses where appropriate\n",
    "    text = re.sub(r\"(\\)\\s)\", r' \\1', text)\n",
    "    text = re.sub(r\"(\\s\\()\", r'\\1 ', text)\n",
    "\n",
    "    # Separate periods that come before newline or end of string.\n",
    "    text = re.sub('\\. *(\\n|$)', ' . ', text)\n",
    "    \n",
    "    # separate single quotes in the middle of words\n",
    "    # text = re.sub(r\"(\\w)(\\')(\\w)\", r'\\1 \\2 \\3', text)\n",
    "    \n",
    "    # separate out 's at the end of words\n",
    "    text = re.sub(r\"(\\w)(\\'s)(\\s)\", r\"\\1 s \", text)\n",
    "    split_text = text.split()\n",
    "    \n",
    "    return split_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data_list = []\n",
    "for row in data_list:\n",
    "    new_row = [bruces_twitter_tokenizer(row[0].lower()), row[1]]\n",
    "    tokenized_data_list.append(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_data_list[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting each text item to a featureset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways we could convert each response to a featureset. To start, we are essentially going to use a sort of vector space representation. We will are going to create a vocabulary, as we have done before. Then each feature will be the presence or absence of each item in our vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we build a stop list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"lists/stop-words_english_5_en.txt\")\n",
    "stop_list = f.read().split(\"\\n\")\n",
    "stop_list += list('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~â€™')\n",
    "stop_list += list(\"abcdefghijklmnopqrstuvwxyz0123456789\")\n",
    "stop_list = set(stop_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a frequency distribution, ignoring words on the stop list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "word_fdist = nltk.FreqDist() # the corpus frequences\n",
    "for row in tokenized_data_list:\n",
    "    for word in row[0]:\n",
    "        if not word in stop_list:\n",
    "            word_fdist[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our vocabulary will be the 100 most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = word_fdist.most_common(100)\n",
    "vocab = [w[0] for w in mc]\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_feature_extractor(the_text, vocab):\n",
    "    features = {}\n",
    "    for word in vocab:\n",
    "        features['contains(%s)' % word] = (word in the_text)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_feature_extractor(tokenized_data_list[0][0], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_featuresets = []\n",
    "for row in tokenized_data_list:\n",
    "    new_lf = [word_feature_extractor(row[0], vocab), row[1]]\n",
    "    labeled_featuresets.append(new_lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labeled_featuresets[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate all of the labeled featuresets into a test set and a training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to reserve just about 10% of the data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = labeled_featuresets[:400]\n",
    "train_set = labeled_featuresets[400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza_classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.classify.accuracy(pizza_classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_list = [t[1] for t in test_set]\n",
    "test_list = [pizza_classifier.classify(t[0]) for t in test_set]\n",
    "cm = nltk.ConfusionMatrix(gold_list, test_list)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pizza_classifier = nltk.DecisionTreeClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.classify.accuracy(dt_pizza_classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_list = [t[1] for t in test_set]\n",
    "test_list = [dt_pizza_classifier.classify(t[0]) for t in test_set]\n",
    "cm = nltk.ConfusionMatrix(gold_list, test_list)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a slighly different feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_feature_extractor_with_length(the_text, vocab):\n",
    "    features = {}\n",
    "    for word in vocab:\n",
    "        features['contains(%s)' % word] = (word in the_text)\n",
    "    if len(the_text) > 25:\n",
    "        features[\"long\"] = True\n",
    "    else:\n",
    "        features[\"long\"] = False\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_featuresets = []\n",
    "for row in tokenized_data_list:\n",
    "    new_lf = [word_feature_extractor_with_length(row[0], vocab), row[1]]\n",
    "    labeled_featuresets.append(new_lf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = labeled_featuresets[:400]\n",
    "train_set = labeled_featuresets[400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza_classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.classify.accuracy(pizza_classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_list = [t[1] for t in test_set]\n",
    "test_list = [pizza_classifier.classify(t[0]) for t in test_set]\n",
    "cm = nltk.ConfusionMatrix(gold_list, test_list)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "260.1630554199219px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
