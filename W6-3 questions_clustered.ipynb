{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're going to learn how to cluster text.\n",
    "\n",
    "For this I'm going to first use a small database of questions\n",
    "that students submitted to me in advance of a discussion of one of my papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Read it in, tokenize it, get a vocabulary, yadda yadda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re, nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "contraction_patterns = re.compile(r\"(?i)(.)('ll|'re|'ve|n't|'s|'m|'d)\\b\")\n",
    "def is_contraction(the_text):\n",
    "        return contraction_patterns.search(the_text)\n",
    "    \n",
    "def alpha_only (ltext):\n",
    "    return [w.lower() for w in ltext if (len(w) > 0) and (w.isalpha() or w[0]=='<' or is_contraction(w))]\n",
    "\n",
    "raw_file = open('corpora/student_questions.txt').read()\n",
    "question_list = re.findall(r\"(.*?)\\n\", raw_file)\n",
    "question_corpus = []\n",
    "for (i, question) in enumerate(question_list):\n",
    "    question_corpus.append(alpha_only(nltk.word_tokenize(question)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "stop_f = open(\"lists/stop-words_english_5_en.txt\")\n",
    "stop_list = stop_f.read().split(\"\\n\")\n",
    "stop_list += list('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~â€™')\n",
    "stop_list += list(\"abcdefghijklmnopqrstuvwxyz0123456789\")\n",
    "stop_list = set(stop_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "set_vocab = set([])\n",
    "for question in question_corpus:\n",
    "    set_vocab = set_vocab.union(set(question))\n",
    "pruned_vocab = set(sorted([w for w in list(set_vocab) if w not in stop_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "word_fdist = nltk.FreqDist() # the corpus frequences\n",
    "doc_fdist = nltk.FreqDist()# the document frequencies\n",
    "for word in pruned_vocab:\n",
    "    word_fdist[word] = 0\n",
    "    doc_fdist[word] = 0\n",
    "    for question in question_corpus:\n",
    "        if word in question:\n",
    "            doc_fdist[word] += 1\n",
    "            word_fdist[word] += question.count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "vocab_list = [w[0] for w in word_fdist.most_common(500)]\n",
    "print(vocab_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def norm_vec(vec):\n",
    "    mag = np.dot(vec, vec)\n",
    "    if mag == 0:\n",
    "        return vec\n",
    "    else:\n",
    "        return(vec / np.sqrt(mag))\n",
    "    \n",
    "def pure_tf(tf, df, cf, N):\n",
    "    return tf\n",
    "\n",
    "def tf(tf, df, cf, N):\n",
    "    if tf == 0:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = (1 + np.log(tf))\n",
    "    return result\n",
    "\n",
    "def tfidf(tf, df, cf, N):\n",
    "    if tf == 0 or df == 0:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = (1 + np.log(tf)) / df\n",
    "    return result\n",
    "\n",
    "def weight_factor2(tf, df, cf, N):\n",
    "    if tf == 0:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = (1 + np.log(tf))\n",
    "    return result\n",
    "\n",
    "def weighted_word(the_text, word):\n",
    "    return tf(the_text.count(word), doc_fdist[word], word_fdist[word], len(question_corpus))\n",
    "\n",
    "def compute_doc_vector(word_list):\n",
    "    return norm_vec([weighted_word(word_list, word) for word in vocab_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_vectors = []\n",
    "for question in question_corpus:\n",
    "    question_vectors.append(compute_doc_vector(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Put the vectors in a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_matrix = np.zeros([len(vocab_list), len(question_vectors)])\n",
    "for (i, vec) in enumerate(question_vectors):\n",
    "    td_matrix[:, i] = vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_matrix.shape\n",
    "X = td_matrix.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "clustering = AgglomerativeClustering(n_clusters=5, linkage=\"ward\").fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try something tricky, if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthogonalize(vectors):\n",
    "    total_v = np.zeros(len(vectors[0]))\n",
    "    for vec in vectors:\n",
    "            total_v = total_v + vec\n",
    "    total_v = norm_vec(total_v)\n",
    "    new_doc_vectors = []\n",
    "    for v in vectors:\n",
    "        new_doc_vectors.append(norm_vec(v - np.dot(v, total_v) * total_v))\n",
    "    return new_doc_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_v = np.zeros(len(question_vectors[0]))\n",
    "for vec in question_vectors:\n",
    "    total_v = total_v + vec\n",
    "total_v = norm_vec(total_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho_vectors = orthogonalize(question_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oclustering = AgglomerativeClustering(n_clusters=5, linkage=\"ward\").fit(ortho_vectors)\n",
    "oclustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "def get_centroids(X, clustering):\n",
    "    clf = NearestCentroid()\n",
    "    clf.fit(X, clustering.labels_)\n",
    "    centroids = clf.centroids_\n",
    "    return centroids\n",
    "\n",
    "centroids = get_centroids(question_vectors, oclustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words_from_centroid(centroids, n, to_print=10, printit=True):\n",
    "    sc = list(np.argsort(centroids[n]))\n",
    "    sc.reverse()\n",
    "    result = []\n",
    "    for i in range(to_print):\n",
    "        if printit:\n",
    "            print(vocab_list[sc[i]], round(centroids[n][sc[i]], 3))\n",
    "        result.append([vocab_list[sc[i]], round(centroids[n][sc[i]], 3)])\n",
    "    return result\n",
    "\n",
    "def top_words_from_centroids(centroids, to_print=10, printit=False):\n",
    "    result = []\n",
    "    for n in range(len(centroids)):\n",
    "        if printit:\n",
    "            print(\"Cluster \", n)\n",
    "            top_words_from_centroid(centroids, n, to_print, printit=printit)\n",
    "            print(\"\\n\")\n",
    "        result.append(top_words_from_centroid(centroids, n, to_print, printit=printit))\n",
    "    return result\n",
    "        \n",
    "class ListTable(list):\n",
    "    def _repr_html_(self):\n",
    "        html = [\"<table style= 'border: 1px solid black; display:inline-block'>\"]\n",
    "        for row in self:\n",
    "            html.append(\"<tr>\")\n",
    "            for col in row:\n",
    "                html.append(\"<td align='left' style='border: .5px solid gray;'>{0}</td>\".format(col))\n",
    "            \n",
    "            html.append(\"</tr>\")\n",
    "        html.append(\"</table>\")\n",
    "        return ''.join(html)\n",
    "    \n",
    "class MultiTable(list):\n",
    "    def _repr_html_(self):\n",
    "        html = []\n",
    "        for l in self:\n",
    "            html.append(\"<table style= 'border: 1px solid black; display:inline-block; margin-right: 10px;'>\")\n",
    "            for row in l:\n",
    "                html.append(\"<tr>\")\n",
    "                for col in row:\n",
    "                    html.append(\"<td align='left' style='border: .5px solid gray;'>{0}</td>\".format(col))\n",
    "\n",
    "                html.append(\"</tr>\")\n",
    "            html.append(\"</table>\")\n",
    "        return ''.join(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiTable(top_words_from_centroids(centroids, printit=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_questions(cluster_number):\n",
    "    res = []\n",
    "    for n, i in enumerate(oclustering.labels_):\n",
    "        if i == cluster_number:\n",
    "            res.append([n, question_list[n]])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ListTable(print_questions(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
