{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from numpy import dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_vec(v):\n",
    "    return v / np.linalg.norm(v)\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>The Seasons Corpus</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bruce: Go to slides here to show examples from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from seasons_module import load_seasons_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seasons_corpus = load_seasons_corpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seasons_corpus[\"angelapre\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile the vocabulary**\n",
    "\n",
    "This is every unique word in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_vocab = set([])\n",
    "for fname in seasons_corpus.keys():\n",
    "    set_vocab = set_vocab.union(set(seasons_corpus[fname][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in a stop list. Then remove all of these words from the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"lists/seasons_stop_list.txt\")\n",
    "stop_list = set(f.read().split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_vocab = set(sorted([w for w in list(set_vocab) if w not in stop_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pruned_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the document vector for each document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc_vectors = {}\n",
    "for fname in seasons_corpus.keys():\n",
    "    doc_vectors[fname] = np.array([seasons_corpus[fname][0].count(word) for word in pruned_vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc_vectors[\"angelapre\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize the vectors**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the vectors\n",
    "for fname, vec in doc_vectors.items():\n",
    "    doc_vectors[fname] = norm_vec(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc_vectors[\"angelapre\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare some pairs of students**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_students(s1, s2):\n",
    "    return round(dot(doc_vectors[s1], doc_vectors[s2]), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compare_students('alipre', 'vanessapre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tab = ListTable()\n",
    "tab.append([\"name\", \"similarity\", \"code\"])\n",
    "for name in doc_vectors.keys():\n",
    "    tab.append([name, str(compare_students(name, 'angelapre')), seasons_corpus[name][1]])\n",
    "tab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compare to pre-written comparison documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from seasons_module import load_seasons_comparison_files\n",
    "comparison_dict = load_seasons_comparison_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute vectors for the comparison documents\n",
    "comparison_vectors = {}\n",
    "for fname in comparison_dict.keys():\n",
    "    comparison_vectors[fname] = norm_vec(np.array([comparison_dict[fname].count(word) for word in pruned_vocab])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compare_to_compvecs(s1):\n",
    "    resultdict = {}\n",
    "    for cname in comparison_vectors.keys():\n",
    "        resultdict[cname] = dot(doc_vectors[s1], comparison_vectors[cname])\n",
    "    return resultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compare_to_compvecs(\"angelapre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_from_dict(the_dict):\n",
    "    key, value = max(the_dict.items(), key=lambda x:x[1])\n",
    "    return key\n",
    "\n",
    "student_codes = {}\n",
    "for name in doc_vectors.keys():\n",
    "    student_codes[name] = max_from_dict(compare_to_compvecs(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How similar are our results to the codes assigned by human coders?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy():\n",
    "    number_right = 0\n",
    "    total_possible = 0\n",
    "    for name in student_codes.keys():\n",
    "        if seasons_corpus[name][1] != \"none\":\n",
    "            total_possible += 1\n",
    "            if student_codes[name] == seasons_corpus[name][1]:\n",
    "                number_right += 1\n",
    "    return 1.0 * number_right / total_possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compute_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gold_list = []\n",
    "test_list = []\n",
    "for name in student_codes.keys():\n",
    "    if seasons_corpus[name][1] != \"none\":\n",
    "        gold_list += [seasons_corpus[name][1]]\n",
    "        test_list += [student_codes[name]]\n",
    "cm = nltk.ConfusionMatrix(gold_list, test_list)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some slightly different ways of computing document vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First variant: use just a subset of the vocabulary when constructing the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_fdist = nltk.FreqDist()\n",
    "for fname in seasons_corpus.keys():\n",
    "    pruned_transcript_words = [w for w in seasons_corpus[fname][0] if w not in stop_list]\n",
    "    word_fdist.update(pruned_transcript_words)\n",
    "word_fdist.most_common(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vocab = [w[0] for w in word_fdist.most_common(50) if w not in stop_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the document vector for each document\n",
    "doc_vectors = {}\n",
    "for fname in seasons_corpus.keys():\n",
    "    doc_vectors[fname] = norm_vec(np.array([seasons_corpus[fname][0].count(word) for word in new_vocab]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute vectors for the comparison documents\n",
    "comparison_vectors = {}\n",
    "for fname in comparison_dict.keys():\n",
    "    comparison_vectors[fname] = norm_vec(np.array([comparison_dict[fname].count(word) for word in new_vocab]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_codes = {}\n",
    "for name in doc_vectors.keys():\n",
    "    student_codes[name] = max_from_dict(compare_to_compvecs(name))\n",
    "compute_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_list = []\n",
    "test_list = []\n",
    "for name in student_codes.keys():\n",
    "    if seasons_corpus[name][1] != \"none\":\n",
    "        gold_list += [seasons_corpus[name][1]]\n",
    "        test_list += [student_codes[name]]\n",
    "cm = nltk.ConfusionMatrix(gold_list, test_list)\n",
    "cm\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other variants: Use different weight factors when constructing the vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A weight factor function will commonly use these different quantities in some combination\n",
    "\n",
    "* `tf = term frequency` (number of times the term appears in the present document)\n",
    "* `df = document frequency` (number of documents in which the term appears)\n",
    "* `cf = corpus frequency` (total number of times the term appears in the entire corpus)\n",
    "* `N = number of documents`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(tf, df, cf, N):\n",
    "    return tf\n",
    "\n",
    "def logtf(tf, df, cf, N):\n",
    "    if tf == 0:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = (1 + np.log(tf))\n",
    "    return result\n",
    "\n",
    "def onehot(tf, df, cf, N):\n",
    "    if tf == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def tfidf(tf, df, cf, N):\n",
    "    if tf == 0:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = (1 + np.log(tf)) * np.log(N  / df)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We need to construct the document frequency distribution since we don't have that yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_fdist = nltk.FreqDist()\n",
    "for fname in seasons_corpus.keys():\n",
    "    pruned_transcript_words = [w for w in seasons_corpus[fname][0] if w not in stop_list]\n",
    "    doc_fdist.update(list(set(pruned_transcript_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A little function to simplify the task of constructing vectors with different weight factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vector(words, vocab, df, cf, N, weight_function):\n",
    "    new_vector = []\n",
    "    for w in vocab:\n",
    "        tf = words.count(w)\n",
    "        new_vector.append(weight_function(tf, df[w], cf[w], N))\n",
    "    return norm_vec(np.array(new_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the document vector for each document\n",
    "doc_vectors = {}\n",
    "N = len(seasons_corpus.keys())\n",
    "wf = tf\n",
    "for fname in seasons_corpus.keys():\n",
    "    doc_vectors[fname] = compute_vector(seasons_corpus[fname][0], new_vocab, doc_fdist, word_fdist, N, wf)\n",
    "# Compute vectors for the comparison documents\n",
    "comparison_vectors = {}\n",
    "for fname in comparison_dict.keys():\n",
    "    comparison_vectors[fname] = compute_vector(comparison_dict[fname], new_vocab, doc_fdist, word_fdist, N, wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_codes = {}\n",
    "for name in doc_vectors.keys():\n",
    "    student_codes[name] = max_from_dict(compare_to_compvecs(name))\n",
    "compute_accuracy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
