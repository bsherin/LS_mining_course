{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasons in Pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read it in, as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seasons_module import load_seasons_corpus\n",
    "seasons_corpus = load_seasons_corpus()\n",
    "fnames = list(seasons_corpus.keys())\n",
    "docs = [seasons_corpus[fname][0] for fname in fnames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break each transcript into a list of overlapping lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_segment(tokenized_text, segment_size, step_size):\n",
    "    result = []\n",
    "    maxv = len(tokenized_text)\n",
    "    for left in range(0, maxv, step_size):\n",
    "        result.append(tokenized_text[left:(left + segment_size)])\n",
    "        if ((left + segment_size) >= maxv):\n",
    "            break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_segments = []\n",
    "for doc in docs:\n",
    "    all_segments += sliding_segment(doc, 100, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the vocabulary, treating each segment as a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_vocab = set([])\n",
    "for seg in all_segments:\n",
    "    set_vocab = set_vocab.union(set(seg))\n",
    "f = open(\"lists/stoplist3k2sw.txt\")\n",
    "stop_list = set(f.read().split(\"\\n\"))\n",
    "pruned_vocab = set(sorted([w for w in list(set_vocab) if w not in stop_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "word_fdist = nltk.FreqDist() # the corpus frequences\n",
    "doc_fdist = nltk.FreqDist()# the document frequencies\n",
    "for word in pruned_vocab:\n",
    "    word_fdist[word] = 0\n",
    "    doc_fdist[word] = 0\n",
    "    for seg in all_segments:\n",
    "        if word in seg:\n",
    "            doc_fdist[word] += 1\n",
    "            word_fdist[word] += seg.count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = [w[0] for w in word_fdist.most_common(500)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the vector for each segment and put them in a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def norm_vec(vec):\n",
    "    mag = np.dot(vec, vec)\n",
    "    if mag == 0:\n",
    "        return vec\n",
    "    else:\n",
    "        return(vec / np.sqrt(mag))\n",
    "    \n",
    "def pure_tf(tf, df, cf, N):\n",
    "    return tf\n",
    "\n",
    "def tf(tf, df, cf, N):\n",
    "    if tf == 0:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = (1 + np.log(tf))\n",
    "    return result\n",
    "\n",
    "def tfidf(tf, df, cf, N):\n",
    "    if tf == 0 or df == 0:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = (1 + np.log(tf)) / df\n",
    "    return result\n",
    "\n",
    "def weight_factor2(tf, df, cf, N):\n",
    "    if tf == 0:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = (1 + np.log(tf))\n",
    "    return result\n",
    "\n",
    "def weighted_word(the_text, word):\n",
    "    return tf(the_text.count(word), doc_fdist[word], word_fdist[word], len(seasons_corpus.keys()))\n",
    "\n",
    "def compute_doc_vector(word_list):\n",
    "    return norm_vec([weighted_word(word_list, word) for word in vocab_list])\n",
    "\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "def get_centroids(X, clustering):\n",
    "    clf = NearestCentroid()\n",
    "    clf.fit(X, clustering.labels_)\n",
    "    centroids = clf.centroids_\n",
    "    return centroids\n",
    "\n",
    "def top_words_from_centroid(centroids, n, to_print=10, printit=True):\n",
    "    sc = list(np.argsort(centroids[n]))\n",
    "    sc.reverse()\n",
    "    result = []\n",
    "    for i in range(to_print):\n",
    "        if printit:\n",
    "            print(vocab_list[sc[i]], round(centroids[n][sc[i]], 3))\n",
    "        result.append([vocab_list[sc[i]], round(centroids[n][sc[i]], 3)])\n",
    "    return result\n",
    "\n",
    "def top_words_from_centroids(centroids, to_print=10, printit=False):\n",
    "    result = []\n",
    "    for n in range(len(centroids)):\n",
    "        if printit:\n",
    "            print(\"Cluster \", n)\n",
    "            top_words_from_centroid(centroids, n, to_print, printit=printit)\n",
    "            print(\"\\n\")\n",
    "        result.append(top_words_from_centroid(centroids, n, to_print, printit=printit))\n",
    "    return result\n",
    "        \n",
    "class ListTable(list):\n",
    "    def _repr_html_(self):\n",
    "        html = [\"<table style= 'border: 1px solid black; display:inline-block'>\"]\n",
    "        for row in self:\n",
    "            html.append(\"<tr>\")\n",
    "            for col in row:\n",
    "                html.append(\"<td align='left' style='border: .5px solid gray;'>{0}</td>\".format(col))\n",
    "            \n",
    "            html.append(\"</tr>\")\n",
    "        html.append(\"</table>\")\n",
    "        return ''.join(html)\n",
    "    \n",
    "class MultiTable(list):\n",
    "    def _repr_html_(self):\n",
    "        html = []\n",
    "        for l in self:\n",
    "            html.append(\"<table style= 'border: 1px solid black; display:inline-block; margin-right: 10px;'>\")\n",
    "            for row in l:\n",
    "                html.append(\"<tr>\")\n",
    "                for col in row:\n",
    "                    html.append(\"<td align='left' style='border: .5px solid gray;'>{0}</td>\".format(col))\n",
    "\n",
    "                html.append(\"</tr>\")\n",
    "            html.append(\"</table>\")\n",
    "        return ''.join(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectors = []\n",
    "for seg in all_segments:\n",
    "    doc_vectors.append(compute_doc_vector(seg))\n",
    "td_matrix = np.zeros([len(vocab_list), len(all_segments)])\n",
    "for (i, vec) in enumerate(doc_vectors):\n",
    "    td_matrix[:, i] = vec\n",
    "X = td_matrix.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "clustering = AgglomerativeClustering(n_clusters=7).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = get_centroids(X, clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiTable(top_words_from_centroids(centroids, printit=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def orthogonalize(vectors):\n",
    "    total_v = np.zeros(len(vectors[0]))\n",
    "    for vec in vectors:\n",
    "            total_v = total_v + vec\n",
    "    total_v = norm_vec(total_v)\n",
    "    new_doc_vectors = []\n",
    "    for v in vectors:\n",
    "        new_doc_vectors.append(norm_vec(v - np.dot(v, total_v) * total_v))\n",
    "    return new_doc_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ortho_vectors = orthogonalize(doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_matrix = np.zeros([len(vocab_list), len(all_segments)])\n",
    "for (i, vec) in enumerate(ortho_vectors):\n",
    "    td_matrix[:, i] = vec\n",
    "ortho_X = td_matrix.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(n_clusters=7, linkage=\"ward\", affinity=\"euclidean\").fit(ortho_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = get_centroids(X, clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiTable(top_words_from_centroids(centroids, printit=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kclustering = KMeans(n_clusters=7).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiTable(top_words_from_centroids(kclustering.cluster_centers_, printit=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reapply to transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster the segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = AgglomerativeClustering(n_clusters=7, linkage=\"ward\", affinity=\"euclidean\").fit(X)\n",
    "centroids = get_centroids(X, clustering)\n",
    "MultiTable(top_words_from_centroids(centroids, printit=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a matrix of the centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_centroids = [norm_vec(centroid) for centroid in centroids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_matrix = np.zeros([len(normalized_centroids),len(normalized_centroids[0])])\n",
    "for i in range(len(normalized_centroids)):\n",
    "    centroid_matrix[i] = normalized_centroids[i]\n",
    "centroid_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resegment the transcript of one student and compute vectors for the segments\n",
    "Put them in a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_doc = seasons_corpus[\"angelapre\"][0]\n",
    "segmented_student = sliding_segment(student_doc, 100, 25)\n",
    "segment_vectors = []\n",
    "for seg in segmented_student:\n",
    "    segment_vectors.append(compute_doc_vector(seg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_matrix = np.zeros([len(segment_vectors[0]), len(segment_vectors)])\n",
    "for i in range(len(segment_vectors)):\n",
    "    segment_matrix[:, i] = segment_vectors[i]\n",
    "segment_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiply the two matrices to code each segment (sort of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "the_array = np.dot(centroid_matrix, segment_matrix)\n",
    "the_array.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a heatmap of the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we give a name to each cluster that is the top three words in the cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_word_lists = top_words_from_centroids(centroids, printit=False)\n",
    "labels = []\n",
    "for clus in top_word_lists:\n",
    "    new_label = \"{}-{}-{}\".format(clus[0][0], clus[1][0], clus[2][0])\n",
    "    labels.append(new_label)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.cm as cm\n",
    "fig = matplotlib.pyplot.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "dialogs = []\n",
    "(nrows, ncols) = the_array.shape\n",
    "cax = ax.imshow(the_array, cmap=cm.gist_yarg, aspect=\"auto\", interpolation='nearest')\n",
    "\n",
    "ind = np.arange(ncols)\n",
    "ax.set_xticks(ind, minor=False)\n",
    "ax.set_xticks(ind + .5, minor=True)\n",
    "ax.get_xaxis().set_ticklabels(ind + 1, size=\"x-small\")\n",
    "\n",
    "ind = np.arange(nrows)\n",
    "\n",
    "ax.set_yticks(ind, minor=False)\n",
    "ax.set_yticks(ind + .5, minor=True)\n",
    "ax.get_yaxis().set_ticklabels(labels, size=\"small\", rotation=\"horizontal\")\n",
    "\n",
    "ax.grid(True, which='minor', linestyle=':')\n",
    "\n",
    "fig.set_facecolor(\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
