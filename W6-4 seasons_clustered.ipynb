{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasons Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, nltk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the corpus and get the vocabulary, as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seasons_module import load_seasons_corpus\n",
    "seasons_corpus = load_seasons_corpus()\n",
    "fnames = list(seasons_corpus.keys())\n",
    "docs = [seasons_corpus[fname][0] for fname in fnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_vocab = set([])\n",
    "for doc in docs:\n",
    "    set_vocab = set_vocab.union(set(doc))\n",
    "f = open(\"lists/seasons_stop_list.txt\")\n",
    "stop_list = set(f.read().split(\"\\n\"))\n",
    "pruned_vocab = set(sorted([w for w in list(set_vocab) if w not in stop_list]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "word_fdist = nltk.FreqDist() # the corpus frequences\n",
    "doc_fdist = nltk.FreqDist()# the document frequencies\n",
    "for word in pruned_vocab:\n",
    "    word_fdist[word] = 0\n",
    "    doc_fdist[word] = 0\n",
    "    for doc in docs:\n",
    "        if word in doc:\n",
    "            doc_fdist[word] += 1\n",
    "            word_fdist[word] += doc.count(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = [w[0] for w in word_fdist.most_common(500)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def norm_vec(vec):\n",
    "    mag = np.dot(vec, vec)\n",
    "    if mag == 0:\n",
    "        return vec\n",
    "    else:\n",
    "        return(vec / np.sqrt(mag))\n",
    "    \n",
    "def pure_tf(tf, df, cf, N):\n",
    "    return tf\n",
    "\n",
    "def tf(tf, df, cf, N):\n",
    "    if tf == 0:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = (1 + np.log(tf))\n",
    "    return result\n",
    "\n",
    "def tfidf(tf, df, cf, N):\n",
    "    if tf == 0 or df == 0:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = (1 + np.log(tf)) / df\n",
    "    return result\n",
    "\n",
    "def weight_factor2(tf, df, cf, N):\n",
    "    if tf == 0:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = (1 + np.log(tf))\n",
    "    return result\n",
    "\n",
    "def weighted_word(the_text, word):\n",
    "    return tf(the_text.count(word), doc_fdist[word], word_fdist[word], len(seasons_corpus.keys()))\n",
    "\n",
    "def compute_doc_vector(word_list):\n",
    "    return norm_vec([weighted_word(word_list, word) for word in vocab_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vectors = []\n",
    "for doc in docs:\n",
    "    doc_vectors.append(compute_doc_vector(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the termxdocument matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_matrix = np.zeros([len(vocab_list), len(fnames)])\n",
    "for (i, vec) in enumerate(doc_vectors):\n",
    "    td_matrix[:, i] = vec\n",
    "X = td_matrix.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "clustering = AgglomerativeClustering(n_clusters=7).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "def get_centroids(X, clustering):\n",
    "    clf = NearestCentroid()\n",
    "    clf.fit(X, clustering.labels_)\n",
    "    centroids = clf.centroids_\n",
    "    return centroids\n",
    "\n",
    "centroids = get_centroids(X, clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_words_from_centroid(centroids, n, to_print=10, printit=True):\n",
    "    sc = list(np.argsort(centroids[n]))\n",
    "    sc.reverse()\n",
    "    result = []\n",
    "    for i in range(to_print):\n",
    "        if printit:\n",
    "            print(vocab_list[sc[i]], round(centroids[n][sc[i]], 3))\n",
    "        result.append([vocab_list[sc[i]], round(centroids[n][sc[i]], 3)])\n",
    "    return result\n",
    "\n",
    "def top_words_from_centroids(centroids, to_print=10, printit=False):\n",
    "    result = []\n",
    "    for n in range(len(centroids)):\n",
    "        if printit:\n",
    "            print(\"Cluster \", n)\n",
    "            top_words_from_centroid(centroids, n, to_print, printit=printit)\n",
    "            print(\"\\n\")\n",
    "        result.append(top_words_from_centroid(centroids, n, to_print, printit=printit))\n",
    "    return result\n",
    "        \n",
    "class ListTable(list):\n",
    "    def _repr_html_(self):\n",
    "        html = [\"<table style= 'border: 1px solid black; display:inline-block'>\"]\n",
    "        for row in self:\n",
    "            html.append(\"<tr>\")\n",
    "            for col in row:\n",
    "                html.append(\"<td align='left' style='border: .5px solid gray;'>{0}</td>\".format(col))\n",
    "            \n",
    "            html.append(\"</tr>\")\n",
    "        html.append(\"</table>\")\n",
    "        return ''.join(html)\n",
    "    \n",
    "class MultiTable(list):\n",
    "    def _repr_html_(self):\n",
    "        html = []\n",
    "        for l in self:\n",
    "            html.append(\"<table style= 'border: 1px solid black; display:inline-block; margin-right: 10px;'>\")\n",
    "            for row in l:\n",
    "                html.append(\"<tr>\")\n",
    "                for col in row:\n",
    "                    html.append(\"<td align='left' style='border: .5px solid gray;'>{0}</td>\".format(col))\n",
    "\n",
    "                html.append(\"</tr>\")\n",
    "            html.append(\"</table>\")\n",
    "        return ''.join(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiTable(top_words_from_centroids(centroids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for n, fname in enumerate(fnames):\n",
    "    res.append([fname, clustering.labels_[n]])\n",
    "ListTable(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine some metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.cluster import SilhouetteVisualizer \n",
    "from sklearn.cluster import KMeans\n",
    "visualizer = SilhouetteVisualizer(KMeans(n_clusters=7))\n",
    "visualizer.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MultiTable(top_words_from_centroids(visualizer.cluster_centers_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
