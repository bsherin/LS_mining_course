{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network of relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "from string import punctuation\n",
    "from itertools import combinations\n",
    "import nltk\n",
    "import numpy as np\n",
    "from IPython.display import display, clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in the civil war corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipediaapi\n",
    "pages = [\n",
    "    \"American Civil War\",\n",
    "    \"Abraham Lincoln\",\n",
    "    \"Slavery in the United States\",\n",
    "    \"Slave states and free states\",\n",
    "    \"Emancipation Proclamation\",\n",
    "    \"Robert E. Lee\",\n",
    "    \"Ulysses S. Grant\",\n",
    "    \"Conclusion of the American Civil War\",\n",
    "    \"Origins of the American Civil War\",\n",
    "    \"Issues of the American Civil War\"\n",
    "]\n",
    "import re\n",
    "\n",
    "def underscorize(pagename):\n",
    "    return re.sub(\" \", \"_\", pagename)\n",
    "\n",
    "wiki_wiki = wikipediaapi.Wikipedia(\n",
    "        language='en',\n",
    "        extract_format=wikipediaapi.ExtractFormat.WIKI\n",
    ")\n",
    "page_dict = {}\n",
    "for page in pages:\n",
    "    pagename = underscorize(page)\n",
    "    print(pagename)\n",
    "    p_wiki = wiki_wiki.page(pagename)\n",
    "    page_text = p_wiki.text.split(\"\\n\")\n",
    "    page_paras = [para for para in page_text if len(para) > 1]\n",
    "    page_dict[pagename] = page_paras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the named entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag every word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_paragraph_sentences(para):\n",
    "    sentences = nltk.sent_tokenize(para)\n",
    "    tagged_sentences = []\n",
    "    for sent in sentences:\n",
    "        tokenized_sentence = nltk.word_tokenize(sent)\n",
    "        tagged_sentence = nltk.pos_tag(tokenized_sentence)\n",
    "        tagged_sentences.append(tagged_sentence)\n",
    "    return tagged_sentences\n",
    "tagged_sentences = []\n",
    "for name, page in page_dict.items():\n",
    "    for para in page:\n",
    "        tagged_sentences += tag_paragraph_sentences(para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the chunks using the nltk named-entity chunker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_sentences = []\n",
    "for n, sent in enumerate(tagged_sentences):\n",
    "    if n % 500 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print('Sentence {} of {}'.format(n, len(tagged_sentences)))\n",
    "    chunked_sentences.append(nltk.ne_chunk(sent))\n",
    "clear_output(wait=True)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_to_tuple(t):\n",
    "    return tuple([t.label(), \" \".join([token for token, pos in t.leaves()])])\n",
    "\n",
    "def extract_entities(chunked_sentence):\n",
    "    entities = []\n",
    "    for i in chunked_sentence:\n",
    "        if type(i) == nltk.Tree:\n",
    "            entities.append(entity_to_tuple(i))\n",
    "    return entities\n",
    "\n",
    "def tuple_to_string(t):\n",
    "    return t[1] + \"_\" + t[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_entities(chunked_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most common entities.\n",
    "\n",
    "Also create a list that has the list of entities in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_fdist = nltk.FreqDist()\n",
    "ents_list = []\n",
    "for sent in chunked_sentences:\n",
    "    ents = extract_entities(sent)\n",
    "    ents_list.append(ents)\n",
    "    entity_fdist.update(ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_fdist.most_common(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the adjacency matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our \"vocabulary\" will consist of the most common entities.\n",
    "\n",
    "Then we'll compute a vector for each sentence. The elements of the vector will be the count of how many times each of the entities in this vocabulary appeared.\n",
    "\n",
    "We'll use the vectors to build a termxdocument matrix. Then we'll convert this to a termxterm matrix, which we'll use as our adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vector(words, vocab):\n",
    "    new_vector = []\n",
    "    for w in vocab:\n",
    "        tf = words.count(w)\n",
    "        new_vector.append(tf)\n",
    "    return np.array(new_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100\n",
    "vocab = [w[0] for w in entity_fdist.most_common(vocab_size)]\n",
    "doc_vectors = {}\n",
    "N = len(ents_list)\n",
    "entity_vector_list = []\n",
    "for ents in ents_list:\n",
    "    entity_vector_list.append(compute_vector(ents, medium_vocab))\n",
    "td_matrix = np.zeros([len(medium_vocab), len(entity_vector_list)])\n",
    "i = 0\n",
    "for entity_vector in entity_vector_list:\n",
    "    td_matrix[:, i] = entity_vector\n",
    "    i = i + 1\n",
    "for r in range(len(medium_vocab)):\n",
    "    td_matrix[r, :] = td_matrix[r, :]\n",
    "tt_matrix = np.dot(td_matrix, td_matrix.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_weighted_graph_from_Aij(am, vocab):\n",
    "    g = nx.Graph()\n",
    "    entity_strings = [tuple_to_string(tup) for tup in vocab]\n",
    "    for entity_data in vocab:\n",
    "        g.add_node(tuple_to_string(entity_data), type=entity_data[0], name=entity_data[1])\n",
    "    dim = len(am)\n",
    "    for r in range(dim):\n",
    "        for c in range(dim):\n",
    "            if r != c and am[r][c] != 0:\n",
    "                g.add_weighted_edges_from([(entity_strings[r], entity_strings[c], am[r][c])])\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw_g = build_weighted_graph_from_Aij(tt_matrix, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's give different colors to each node depending on which type of entity it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {}\n",
    "for i, n in enumerate(cw_g.nodes):\n",
    "    label_dict[n] = vocab[i][1]\n",
    "weights = [cw_g[u][v]['weight'] / 5 for u, v in cw_g.edges()]\n",
    "color_dict = {\"PERSON\": \"blue\",\n",
    "             \"GPE\": \"green\",\n",
    "             \"ORGANIZATION\": \"red\",\n",
    "             \"LOCATION\": \"orange\",\n",
    "             \"GSP\": \"gray\"}\n",
    "node_colors = []\n",
    "for n in cw_g.nodes():\n",
    "    node_colors.append(color_dict[cw_g.nodes()[n][\"type\"]])\n",
    "    pos = nx.spring_layout(cw_g, k=2)\n",
    "plt.figure(figsize=(30, 30))\n",
    "plt.axis('off')\n",
    "nx.draw_networkx(cw_g, pos=pos, labels=label_dict, node_color=node_colors, width=weights, alpha = .5, font_size = 20, node_size = 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the network to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gexf(cw_g, \"civil_war.gexf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
