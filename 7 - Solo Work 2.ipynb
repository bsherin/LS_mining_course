{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Useful starting stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ListTable(list):\n",
    "    def _repr_html_(self):\n",
    "        html = [\"<table style= 'border: 1px solid black;''>\"]\n",
    "        for row in self:\n",
    "            html.append(\"<tr>\")\n",
    "            for col in row:\n",
    "                html.append(\"<td align='left' style='border: .5px solid gray;''>{0}</td>\".format(col))\n",
    "            \n",
    "            html.append(\"</tr>\")\n",
    "        html.append(\"</table>\")\n",
    "        return ''.join(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocations in Genesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look back at our work on collocations in Genesis.\n",
    "I don't think it was very satisfying though.\n",
    "Here you're going to try to do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by reading in genesis again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mygenesis = nltk.corpus.PlaintextCorpusReader(\"corpora\", 'genesis.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a frequency distribution of bigrams, using one of the approaches we used in Notebook 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For practice, make a plot of part of that distribution, for practice. If you can, do it straight from matplotlib, rather than using `FreqDist.plo()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Notebook 6, we did a so-so job of finding real, interesting collocations in Genesis. Our goal here is to see if we can do any better. As a first step, if you already have some ideas how to do better, go ahead and try them here. I'll give you a cell to work in. But, if you don't have anything you want to try, you can go to the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We essentially used the t statistic as a way to score possible collocations.\n",
    "But there are many other measures that can be used.\n",
    "\n",
    "NLTK provides some tools that make it easy to play with a few potential measures.\n",
    "There's a little tutorial here: http://www.nltk.org/howto/collocations.html\n",
    "\n",
    "Take a look at the tutorial, and figure out how to use some of the alternative measures there to score\n",
    "bigrams in Genesis. Are any of them better than the t statistic? Feel free to combine these measures with anything we tried in class.\n",
    "\n",
    "For continuity with what we have done, you 'll probably want to read in the genesis corpus as I do below. (The third line below is different than in the tutorial. I'll get you started below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(mygenesis.words())\n",
    "### continue from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTMS explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## The machinery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now you're going to do some work with the MTMS corpus. As a first step, you just need to run the code in the next cell. It give you the machinery you need to read in the MTMS corpus in a useful way. It uses python *classes*, which is advanced stuff. You can try to understand it if you like, but it's not necessary at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import os\n",
    "\n",
    "class mtms_document:\n",
    "    def __init__(self, base, filename):\n",
    "        csvfile = open(base + \"/\" + filename)\n",
    "        csv_reader = csv.DictReader(csvfile, delimiter=\",\")\n",
    "        self._filename = filename\n",
    "        self._paragraphs = []\n",
    "        self._sentences = []\n",
    "        self._words = []\n",
    "        for r in csv_reader:\n",
    "            raw_sentences = nltk.sent_tokenize(r[\"text\"])\n",
    "            new_sentences = [nltk.word_tokenize(sent) for sent in raw_sentences]\n",
    "            self._sentences += new_sentences\n",
    "            new_words = []\n",
    "            for sent in new_sentences:\n",
    "                new_words += sent\n",
    "                self._paragraphs.append(new_words)\n",
    "            self._words += new_words\n",
    "        return\n",
    "            \n",
    "    def words(self):\n",
    "        return self._words\n",
    "    \n",
    "    def paragraphs(self):\n",
    "        return self._paragraphs\n",
    "    \n",
    "    def sentences(self):\n",
    "        return self._sentences\n",
    "    \n",
    "    def name(self):\n",
    "        return self._filename\n",
    "    \n",
    "class mtms_reader:\n",
    "    def __init__(self):\n",
    "        self.base = \"corpora/mtms_csv\"\n",
    "        self._name_list = os.listdir(self.base)\n",
    "        self.mtms_docs = {}\n",
    "        self._words = []\n",
    "        self._sentences = []\n",
    "        self._paragraphs = []\n",
    "        self._document_list = []\n",
    "        ndocs = len(name_list)\n",
    "        for n, fname in enumerate(self._name_list):\n",
    "            if n % 100 == 0:\n",
    "                print(\"processing doc {} of {}\".format(n, ndocs))\n",
    "            new_doc = mtms_document(self.base, fname)\n",
    "            self.mtms_docs[fname] = new_doc\n",
    "            self._words += new_doc.words()\n",
    "            self._sentences += new_doc.sentences()\n",
    "            self._paragraphs += new_doc.paragraphs()\n",
    "        return\n",
    "    \n",
    "    def document_names(self):\n",
    "        return self._name_list\n",
    "    \n",
    "    def words(self):\n",
    "        return self._words\n",
    "    \n",
    "    def sentences(self):\n",
    "        return self._sentences\n",
    "    \n",
    "    def paragraphs(self):\n",
    "        return self._paragraphs\n",
    "    \n",
    "    def __getitem__(self, docname):\n",
    "        return self.mtms_docs[docname]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Some basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Read in the corpus\n",
    "Find the most common unigrams and bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mr = mtms_reader()\n",
    "# Find unigrams and bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "See if you can discover any interesting or meaningful collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MTMS corpus was actually assembled for a particular reason.\n",
    "The \"Lexicon Project\" is an international collaboration where researchers from several countries, speaking multiple languages, assembled what are essentiall dictionaries of the language used by math teachers to talk about events in their classroom.\n",
    "\n",
    "Miriam Sherin and the Lexicon team here at Northwestern used surveys and interviews to assemble a lexicon that \n",
    "is supposed to characterize the terms used by middle school math teachers here in the U.S.\n",
    "Most of the lexicon is now available in a file in your lists folder.\n",
    "\n",
    "It's in a slightly complicated form. Each lexicon term has its own row, with a list of phrases separated by commas. The phrases on a row are supposed to be different forms in which a lexicon term might appear. The first entry on a row is the base term. \n",
    "\n",
    "You can open the file and look inside. Note that some lexicon \"terms\" have multiple words in them.\n",
    "\n",
    "The code in the next cell pulls in the lexicon file for you and does some initial processing. In particular, it build three lists for you:\n",
    "\n",
    "* `base_terms`: This is a list of all of the base terms.\n",
    "\n",
    "* `one_word_terms`: A list of just the one-word terms.\n",
    "\n",
    "* `two_word_terms`: A list of just the two-word terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_file = open(\"lists/lexicon.txt\")\n",
    "lexicon_raw_groups = lexicon_file.read().split(\"\\n\")\n",
    "all_lexicon_groups = [g.split(\", \") for g in lexicon_raw_groups]\n",
    "lexicon_dict = {}\n",
    "for group in all_lexicon_groups:\n",
    "    split_group = [tuple(t.split()) for t in group]\n",
    "    if len(split_group[0]) == 1:\n",
    "        group_name = split_group[0][0]\n",
    "        lexicon_dict[group_name] = [t[0] for t in split_group]\n",
    "    else:\n",
    "        group_name = split_group[0]\n",
    "        lexicon_dict[group_name] = split_group\n",
    "base_terms = list(lexicon_dict.keys())\n",
    "one_word_terms = [t for t in base_terms if isinstance(t, str)]\n",
    "two_word_terms = [t for t in base_terms if len(t) == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using one of the base terms, you can look up all of its forms like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_one_word_term = base_terms[0]\n",
    "print(a_one_word_term)\n",
    "lexicon_dict[a_one_word_term]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the reasons for assembling the MTMS corpus was to see if the words identified by the lexicon team are actually used frequently by teachers. (There are reasons that the MTMS corpus is good for this purpose, and reasons that it is not so good.)\n",
    "\n",
    "Do some of this exploration yourself. First, how frequently do the one-word terms in the lexicon appear in the lexicon corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How frequently do the two-word terms appear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Lexicon Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding terms that *should* be on the lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you still have some time, you can work on this: By exploring collocations, you can you identify two-word phrases that you think are candidates that *should* be added to the lexicon?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying co-occurrence patterns among lexicon terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, this is a task that I have to work on this week for an ongoing research project. If you can make progress, that would actually be helpful. The question is this: Are there lexicon terms that seem to co-occur, either in sentences, paragraphs, or documents? If you can make some nice pictures for me, that would be nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
