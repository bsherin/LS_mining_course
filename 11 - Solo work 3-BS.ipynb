{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solo work for week 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word meaning in the seasons corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In class, we built a termxdocument matrix of the seasons data. Each *column* of that matrix corresponded to a document - it was the document vector for that document. In essence, we were understanding each column as the *meaning* of the corresponding document.\n",
    "\n",
    "Now get ready to have your mind blown: Just as we understood each column in the termxdocument matrix as the meaning of a document, we can understand each row in the documnet as a meaning of the corresponding *term*. The idea here is that the meaning of a term can be understood as a union of all of the context in which that word appears.\n",
    "\n",
    "You are going to explore this idea a bit. I actually haven't tried this myself, so I'm not sure anything interesting will fall out. Maybe that's a poor way to design an instructional activity. I think it's more fun this way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To make your lives easier, I have copied over the first parts of notebook 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from seasons_module import load_seasons_corpus\n",
    "seasons_corpus = load_seasons_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compile the vocabulary in the usual way.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_vocab = set([])\n",
    "for fname in seasons_corpus.keys():\n",
    "    set_vocab = set_vocab.union(set(seasons_corpus[fname][0]))\n",
    "f = open(\"lists/seasons_stop_list.txt\")\n",
    "stop_list = set(f.read().split(\"\\n\"))\n",
    "pruned_vocab = set(sorted([w for w in list(set_vocab) if w not in stop_list]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the corpus and document frequency for each term.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "word_fdist = nltk.FreqDist() # the corpus frequences\n",
    "doc_fdist = nltk.FreqDist()# the document frequencies\n",
    "for word in pruned_vocab:\n",
    "    word_fdist[word] = 0\n",
    "    doc_fdist[word] = 0\n",
    "    for name in seasons_corpus.keys():\n",
    "        if word in seasons_corpus[name][0]:\n",
    "            doc_fdist[word] += 1\n",
    "            word_fdist[word] += seasons_corpus[name][0].count(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a very small vocabulary**\n",
    "\n",
    "Just 10 words, to make it more simple to understand what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_vocab = [w[0] for w in word_fdist.most_common(10)]\n",
    "print(small_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute the weighted document vector for each document**\n",
    "\n",
    "Same as before, but now using our smaller vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf(tf, df, cf, N):\n",
    "    return tf\n",
    "\n",
    "def logtf(tf, df, cf, N):\n",
    "    if tf == 0:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = (1 + np.log(tf))\n",
    "    return result\n",
    "\n",
    "def onehot(tf, df, cf, N):\n",
    "    if tf == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def tfidf(tf, df, cf, N):\n",
    "    if tf == 0:\n",
    "        result = 0\n",
    "    else:\n",
    "        result = (1 + np.log(tf)) * np.log(N  / df)\n",
    "    return result\n",
    "\n",
    "def compute_vector(words, vocab, df, N, weight_function):\n",
    "    new_vector = []\n",
    "    for w in vocab:\n",
    "        tf = words.count(w)\n",
    "        new_vector.append(weight_function(tf, df[w], 0, N))\n",
    "    return norm_vec(np.array(new_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def norm_vec(v):\n",
    "    return v / np.linalg.norm(v)\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# compute the document vector for each document\n",
    "doc_vectors = {}\n",
    "N = len(seasons_corpus.keys())\n",
    "wf = tf\n",
    "for fname in seasons_corpus.keys():\n",
    "    doc_vectors[fname] = compute_vector(seasons_corpus[fname][0], small_vocab, doc_fdist, N, wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(doc_vectors), len(doc_vectors['angelapre']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a termxdocument matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a matrix where every row corresponds to a word in the vocabulary, and every column corresponds to a document.\n",
    "\n",
    "Another way to say this: Each column in the matrix is the document vector for a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "td_matrix = np.zeros([len(small_vocab), len(doc_vectors)])\n",
    "i = 0\n",
    "name_index = {}\n",
    "name_list = []\n",
    "for name in doc_vectors.keys():\n",
    "    td_matrix[:, i] = doc_vectors[name]\n",
    "    name_index[name] = i\n",
    "    name_list += [name]\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is where things get new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line below is what we executed to compute a matrix reflecting the documentxdocument similarity. You want to modify this line to that it instead computes a matrix showing the termxterm similarity. Let's call it `tt_matrix`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd_matrix = np.dot(td_matrix.transpose(), td_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have the termxterm similarity, here are some additional things to do. Go as far as you like in these tasks. If you get board of this, you can try another set of tasks I'll put below.\n",
    "\n",
    "1. Look at `tt_matrix`. Are the term similarities what you expected?\n",
    "2. It might help to make a heatmap of the sort we made when looking at word co-occurrences. Try that.\n",
    "3. Above we used only the 10 most common terms. Expand this a bit to include some more terms. You might want to think a bit about what terms would be interesting to compare, and make sure that you have included those terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_matrix = np.dot(td_matrix, td_matrix.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_matrix(the_matrix, prec = 2):\n",
    "    sh = the_matrix.shape\n",
    "    if len(sh) == 1:\n",
    "        for i in range(sh[0]):\n",
    "            the_matrix[i] = round(the_matrix[i], prec)\n",
    "    else:\n",
    "        for i in range(sh[0]):\n",
    "            for j in range(sh[1]):\n",
    "                the_matrix[i, j] = round(the_matrix[i, j], prec)\n",
    "    return the_matrix\n",
    "from sympy import *\n",
    "init_printing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Matrix(round_matrix(tt_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def matrix_heatmap(mtx, name_list):\n",
    "    fig=plt.figure(figsize=(10, 10), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    n = len(name_list)\n",
    "    x_tick_marks = np.arange(n)\n",
    "    y_tick_marks = np.arange(n)\n",
    "    plt.xticks(x_tick_marks, name_list, fontsize=8, rotation=90)\n",
    "    plt.yticks(y_tick_marks, name_list, fontsize=8)\n",
    "    plt.tick_params(\"x\", top=True, labeltop=True, bottom=False, labelbottom=False)\n",
    "    plt.imshow(mtx, norm=matplotlib.colors.LogNorm(), interpolation='nearest', cmap='YlOrBr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_heatmap(tt_matrix, small_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a termxdocument matrics with norming document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_vector_n(words, vocab, df, N, weight_function, norm_vector=True):\n",
    "    new_vector = []\n",
    "    for w in vocab:\n",
    "        tf = words.count(w)\n",
    "        new_vector.append(weight_function(tf, df[w], 0, N))\n",
    "    if norm_vector:\n",
    "        return norm_vec(np.array(new_vector))\n",
    "    else:\n",
    "        return np.array(new_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the document vector for each document\n",
    "doc_vectors = {}\n",
    "N = len(seasons_corpus.keys())\n",
    "wf = tf\n",
    "for fname in seasons_corpus.keys():\n",
    "    doc_vectors[fname] = compute_vector_n(seasons_corpus[fname][0], small_vocab, doc_fdist, N, wf, False)\n",
    "td_matrix = np.zeros([len(small_vocab), len(doc_vectors)])\n",
    "i = 0\n",
    "name_index = {}\n",
    "name_list = []\n",
    "for name in doc_vectors.keys():\n",
    "    td_matrix[:, i] = doc_vectors[name]\n",
    "    name_index[name] = i\n",
    "    name_list += [name]\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the word vectors (the rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in range(len(small_vocab)):\n",
    "    td_matrix[r, :] = norm_vec(td_matrix[r, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_matrix = np.dot(td_matrix, td_matrix.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_heatmap(tt_matrix, small_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try a somewhat larger vocabulary\n",
    "You can set the size of the vocabulary in the next line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medium_vocab = [w[0] for w in word_fdist.most_common(vocab_size)]\n",
    "doc_vectors = {}\n",
    "N = len(seasons_corpus.keys())\n",
    "wf = tf\n",
    "for fname in seasons_corpus.keys():\n",
    "    doc_vectors[fname] = compute_vector_n(seasons_corpus[fname][0], medium_vocab, doc_fdist, N, wf, False)\n",
    "td_matrix = np.zeros([len(medium_vocab), len(doc_vectors)])\n",
    "i = 0\n",
    "name_index = {}\n",
    "name_list = []\n",
    "for name in doc_vectors.keys():\n",
    "    td_matrix[:, i] = doc_vectors[name]\n",
    "    name_index[name] = i\n",
    "    name_list += [name]\n",
    "    i = i + 1\n",
    "for r in range(len(medium_vocab)):\n",
    "    td_matrix[r, :] = norm_vec(td_matrix[r, :])\n",
    "tt_matrix = np.dot(td_matrix, td_matrix.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_heatmap(tt_matrix, medium_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you're really ambitious:\n",
    "\n",
    "4. In this exercise, we are understanding a term's meaning as the collection of contexts in which it appears. So far we have been defining a context simply as the entire document in which the word appears. But there are many other ways we could define a context. The task here is to try some other contexts. One possibility is to look at the collection of utterances in which the word appears. In that case, we'd be treating each utterance as a document when we make are termxdocument matrix. To facilitate this a bit, I made a function that loads the corpus for you, but gives each transcript as a list of tokenized utterances. It's invoked in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seasons_module import load_seasons_corpus_as_utterances\n",
    "seasons_corpus_with_utterances = load_seasons_corpus_as_utterances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_utterances = []\n",
    "for entry in seasons_corpus_with_utterances.values():\n",
    "    all_utterances += entry[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 25\n",
    "medium_vocab = [w[0] for w in word_fdist.most_common(vocab_size)]\n",
    "doc_vectors = {}\n",
    "N = len(seasons_corpus.keys())\n",
    "wf = tf\n",
    "utterance_vector_list = []\n",
    "for utterance in all_utterances:\n",
    "    utterance_vector_list.append(compute_vector_n(utterance, medium_vocab, doc_fdist, N, wf, False))\n",
    "td_matrix = np.zeros([len(medium_vocab), len(utterance_vector_list)])\n",
    "i = 0\n",
    "name_index = {}\n",
    "name_list = []\n",
    "for utterance_vector in utterance_vector_list:\n",
    "    td_matrix[:, i] = utterance_vector\n",
    "    name_index[name] = i\n",
    "    name_list += [name]\n",
    "    i = i + 1\n",
    "for r in range(len(medium_vocab)):\n",
    "    td_matrix[r, :] = norm_vec(td_matrix[r, :])\n",
    "tt_matrix = np.dot(td_matrix, td_matrix.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_heatmap(tt_matrix, medium_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paragraph-paragraph similarity as a measure of text difficulty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's something completely different you can try, if you're board of the seasons corpus.\n",
    "\n",
    "Our task here is to develop a measure of how difficult a text is by looking at the inter-paragraph cohesion. \n",
    "The idea is that we'll:\n",
    "\n",
    "* construct a vector for every sentence or paragraph in a text. \n",
    "* find the similarity between sequential pair of paragraphs\n",
    "* and, finally, we'll find the averages of these similarities.\n",
    "\n",
    "If the average of the similarities is high (i.e., closer to one) then the cohesion is high.\n",
    "\n",
    "I'm think that you'd pick a couple of text from the Gutenberg corpus, and compare their cohesions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step will be to read in one of these corpora. For example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "mycorpus = nltk.corpus.PlaintextCorpusReader(\"corpora/gutenberg\", 'melville-moby_dick.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've got this, you can access the book as paragraphs or sentences. For example, this gives you the paragraphs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mycorpus.paras()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `mycorpus.paras()` give you a list, where each item in the list is a paragraph. \n",
    "But each paragraph is itself a list of sentences.\n",
    "\n",
    "Probably a first step then is to combine the sentences in each paragraph into one long list of words.\n",
    "I guess I'll do that for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs = []\n",
    "for para in mycorpus.paras():\n",
    "    flat_para = []\n",
    "    for sent in para:\n",
    "        flat_para += sent\n",
    "    paragraphs.append(flat_para)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now `paragraphs` is a long list of paragraphs.\n",
    "The next step is to convert each of these paragraphs into a vector\n",
    "\n",
    "First, I'll get a stop list of all of the things we want to ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"lists/stop-words_english_5_en.txt\")\n",
    "stop_list = f.read().split(\"\\n\")\n",
    "stop_list += list('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~’')\n",
    "stop_list += list(\"abcdefghijklmnopqrstuvwxyz0123456789\")\n",
    "stop_list = set(stop_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next get the word and document frequencies, ignoring words on the stop list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "word_fdist = nltk.FreqDist() # the corpus frequences\n",
    "doc_fdist = nltk.FreqDist()# the document frequencies\n",
    "for paragraph in paragraphs:\n",
    "    for wordraw in paragraph:\n",
    "        word = wordraw.lower()\n",
    "        if not word in stop_list:\n",
    "            word_fdist[word] += 1\n",
    "            if word not in doc_fdist:\n",
    "                doc_fdist[word] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the vocabulary, we'll take the 500 most common terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 500\n",
    "vocab = [w[0] for w in word_fdist.most_common(vocab_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now compute the document vector for each document.\n",
    "\n",
    "First, I'm creating a new version of `norm_vec` that watches out for zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_vec(v):\n",
    "    if np.linalg.norm(v) == 0:\n",
    "        return np.zeros(len(v))\n",
    "    else:\n",
    "        return v / np.linalg.norm(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_vectors = []\n",
    "N = len(paragraphs)\n",
    "wf = tf\n",
    "for paragraph in paragraphs:\n",
    "    paragraph_vectors.append(compute_vector(paragraph, vocab, doc_fdist, N, wf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next find the dot product between each successive pair of paragraphs.\n",
    "Then find their average.\n",
    "\n",
    "That's your measure of coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_prods = []\n",
    "for r in range(len(paragraph_vectors) - 1):\n",
    "    dot_prods.append(np.dot(paragraph_vectors[r], paragraph_vectors[r + 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(dot_prods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's make the whole process into a function and do it for a different book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence(book_title):\n",
    "    mycorpus = nltk.corpus.PlaintextCorpusReader(\"corpora/gutenberg\", book_title)\n",
    "    paragraphs = []\n",
    "    for para in mycorpus.paras():\n",
    "        flat_para = []\n",
    "        for sent in para:\n",
    "            flat_para += sent\n",
    "        paragraphs.append(flat_para)\n",
    "    f = open(\"lists/stop-words_english_5_en.txt\")\n",
    "    stop_list = f.read().split(\"\\n\")\n",
    "    stop_list += list('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~’')\n",
    "    stop_list += list(\"abcdefghijklmnopqrstuvwxyz0123456789\")\n",
    "    stop_list = set(stop_list)\n",
    "    word_fdist = nltk.FreqDist() # the corpus frequences\n",
    "    doc_fdist = nltk.FreqDist()# the document frequencies\n",
    "    for paragraph in paragraphs:\n",
    "        for wordraw in paragraph:\n",
    "            word = wordraw.lower()\n",
    "            if not word in stop_list:\n",
    "                word_fdist[word] += 1\n",
    "                if word not in doc_fdist:\n",
    "                    doc_fdist[word] = 1\n",
    "    vocab_size = 100\n",
    "    vocab = [w[0] for w in word_fdist.most_common(vocab_size)]\n",
    "    # compute the document vector for each document\n",
    "    paragraph_vectors = []\n",
    "    N = len(paragraphs)\n",
    "    wf = tf\n",
    "    for paragraph in paragraphs:\n",
    "        paragraph_vectors.append(compute_vector(paragraph, vocab, doc_fdist, N, wf))\n",
    "    dot_prods = []\n",
    "    for r in range(len(paragraph_vectors) - 1):\n",
    "        dot_prods.append(np.dot(paragraph_vectors[r], paragraph_vectors[r + 1]))\n",
    "    print(np.mean(dot_prods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_coherence(\"milton-paradise.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_coherence(\"burgess-busterbrown.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_coherence(\"melville-moby_dick.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
