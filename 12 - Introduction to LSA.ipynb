{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Introduction to Latent Semantic Analysis\n",
    "\n",
    "Currently, our vector space has a very **high dimensionality**, since there is a separate\n",
    "dimension for every single word in the vocabulary. \n",
    "\n",
    "There are a couple of **small problems** with this:\n",
    "\n",
    "* Computations can take a long time.\n",
    "* The vectors are hard to visualize.\n",
    "\n",
    "There is also **one big problem**:\n",
    "* Passages of text that are made up of similar, but non-identical words, can have a dot product of zero.\n",
    "\n",
    "With so many dimensions, there's lots of room for all of the vectors to spread out, so that they have little overlap. But we'd like for passages with similar *meaning* to have a non-zero dot product.\n",
    "\n",
    "There are a number of ways of dealing with this problem. One is **latent semantic analysis**. \n",
    "\n",
    "The primary innovation in LSA is that it uses a procedure called **singular value decomposition** to squish all of those dimensions down into a smaller number of dimensions. This starts to give overlaps where there were none before.\n",
    "\n",
    "Another __innovation typically__ used in LSA is the use of a **training corpus**. I'll try to remember to briefly mention this at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import linalg\n",
    "from IPython.core.display import Image\n",
    "from sympy import *\n",
    "init_printing()\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def round_matrix(the_matrix, prec = 2):\n",
    "    sh = the_matrix.shape\n",
    "    if len(sh) == 1:\n",
    "        for i in range(sh[0]):\n",
    "            the_matrix[i] = round(the_matrix[i], prec)\n",
    "    else:\n",
    "        for i in range(sh[0]):\n",
    "            for j in range(sh[1]):\n",
    "                the_matrix[i, j] = round(the_matrix[i, j], prec)\n",
    "    return the_matrix\n",
    "def show_rounded_matrix(the_matrix):\n",
    "    return Matrix(round_matrix(the_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I'm going to use a very simple example from: \n",
    "\n",
    "**Landauer, T., Peter W. Foltz, and D. Laham. “An Introduction to Latent Semantic Analysis.” *Discourse Processes* 25 (1998): 259–84.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Image(filename=\"images/land-data.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "doc_dict = {\n",
    "    \"c1\": \"Human machine interface for ABC computer applications\",\n",
    "    \"c2\": \"A survey of user opinion of computer system response time\",\n",
    "    \"c3\": \"The EPS user interface management system\",\n",
    "    \"c4\": \"System and human system engineering testing for EPS\",\n",
    "    \"c5\": \"Relation of user perceived response time to error measurement\",\n",
    "    \"m1\": \"The generation of random, binary, ordered trees\",\n",
    "    \"m2\": \"The intersection of graph of photos in trees\",\n",
    "    \"m3\": \"Graph minors IV Widths of trees and well-quasi-ordering\",\n",
    "    \"m4\": \"Graph minors A survey\" \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we'll do an analysis **without** LSA\n",
    "\n",
    "Make a termxdocument matrix in the usual way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"images/land-matrix.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnames = sorted(doc_dict.keys())\n",
    "\n",
    "vocab = ['human', 'interface', 'computer', 'user', 'system', 'response', 'time', 'eps', 'survey', 'trees', 'graph', 'minors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, val in doc_dict.items():\n",
    "    doc_dict[k] = val.lower().split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_matrix = np.zeros([len(vocab), len(dnames)])\n",
    "i = 0\n",
    "for term in vocab:\n",
    "    new_row = [doc_dict[dname].count(term) for dname in dnames]\n",
    "    td_matrix[i, :] = new_row\n",
    "    i = i + 1\n",
    "print(\"td_matrix = \")\n",
    "show_rounded_matrix(td_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the documentxdocument similarities\n",
    "\n",
    "The first step is to normalize the document vectors, which are the columns in the above matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def norm_vec(v):\n",
    "    if np.linalg.norm(v) == 0:\n",
    "        return np.zeros(len(v))\n",
    "    else:\n",
    "        return v / np.linalg.norm(v)\n",
    "\n",
    "def normalize_columns(mat):\n",
    "    mat_norm = np.copy(mat)\n",
    "    for i in range(mat.shape[1]):\n",
    "        mat_norm[:, i] = norm_vec(mat[:, i])\n",
    "    return mat_norm\n",
    "\n",
    "def normalize_rows(mat):\n",
    "    mat_norm = np.copy(mat)\n",
    "    for i in range(mat.shape[0]):\n",
    "        mat_norm[i, :] = norm_vec(mat[i, :])\n",
    "    return mat_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_col_norm = normalize_columns(td_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_rounded_matrix(td_col_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To get docxdoc similarities with do $A^TA$**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_doc_comparisons = np.dot(np.transpose(td_col_norm), td_col_norm)\n",
    "show_rounded_matrix(doc_doc_comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def matrix_heatmap(mtx, name_list):\n",
    "    fig=plt.figure(figsize=(6, 6), dpi= 80, facecolor='w', edgecolor='k')\n",
    "    n = len(name_list)\n",
    "    x_tick_marks = np.arange(n)\n",
    "    y_tick_marks = np.arange(n)\n",
    "    plt.xticks(x_tick_marks, name_list, fontsize=8, rotation=90)\n",
    "    plt.yticks(y_tick_marks, name_list, fontsize=8)\n",
    "    plt.tick_params(\"x\", top=True, labeltop=True, bottom=False, labelbottom=False)\n",
    "    plt.imshow(mtx, norm=matplotlib.colors.LogNorm(), interpolation='nearest', cmap='YlOrBr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_heatmap(doc_doc_comparisons, doc_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term x term comparisons: $AA^T$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As before, we think of the rows of the matrix as representing the terms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_row_norm = normalize_rows(td_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_term_comparisons = np.dot(td_row_norm, np.transpose(td_row_norm))\n",
    "print(\"term x term comparisons =\")\n",
    "show_rounded_matrix(term_term_comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_heatmap(term_term_comparisons, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application: Search for documents that match a query\n",
    "\n",
    "query = \"human interface\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q = \"human interface\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First make a vector for this query and normalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_vec = np.array([q.count(term) for term in vocab], float)\n",
    "q_vec_norm = norm_vec(q_vec)\n",
    "print(q_vec_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then compare this vector to each of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_products = np.dot(td_col_norm.transpose(), q_vec_norm)\n",
    "for i in range(len(dot_products)):\n",
    "    print(dnames[i] + \": \" + str(round(dot_products[i], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's do some LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the SVD to get word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The of LSA is done by the singular value decomposition.\n",
    "\n",
    "SVD is a way of constructing an approximation to a matrix. For our purposes, the key thing to know is that SVD takes a termxdocument matrix and factors it into three matrices. We're going to make use of just one of those matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\huge {A=T_{txn}S_{nxn}(D_{dxn})^T}$\n",
    "\n",
    "$\\large n=min(t,d)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=\"images/lsiexpanded2.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Numpy` will just do the svd for us.\n",
    "\n",
    "We have to decide how many dimensions we want to keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = 2\n",
    "T, S, Dt = linalg.svd(td_matrix, full_matrices = False)\n",
    "T_reduced = T[:, 0:dims]\n",
    "Dt_reduced = Dt[0:dims, :]\n",
    "S_reduced = np.diag(S)[0:dims, 0:dims]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Using the new word vectors to get document vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how we're going to think of the key maneuver here.\n",
    "We're going to think of the rows in `T_reduced` as being a vector for each of the words in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_rounded_matrix(T_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to find the vector for a document, we add the vectors corresponding to each of the words that appear. This will, in this case, give us a two dimensional vector for any document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, here's the unreduced vector for c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c1_vector = td_matrix[:, 0]\n",
    "print(c1_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the squished vector for c1, we add up the word vectors for the first three words in our vocabulary.\n",
    "\n",
    "Here I won't normalize the word vectors first. I'm not sure what's the right thing to do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_reduced[0] + T_reduced[1] + T_reduced[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more efficient way to write this is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(T_reduced.transpose(), c1_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Vectors\n",
    "Since everything is in 2 dimensions, we can make plots.\n",
    "First, we plot the vectors for the words. Here they aren't normalized:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the word vectors\n",
    "\n",
    "Here they aren't normalized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.cla\n",
    "pyplot.scatter(T_reduced[:, 0], T_reduced[:, 1])\n",
    "for i in range(len(vocab)):\n",
    "    pyplot.annotate(vocab[i], T_reduced[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the document vectors\n",
    "\n",
    "Now do the documents. First we find the squished vectors for each document, then plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmatrix = np.dot(T_reduced.transpose(), td_matrix)\n",
    "show_rounded_matrix(dmatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.scatter(dmatrix[0], dmatrix[1])\n",
    "dmatrix_trans = np.transpose(dmatrix)\n",
    "for i in range(len(dnames)):\n",
    "    pyplot.annotate(dnames[i], dmatrix_trans[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should probably look at these normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmatrix_normed = normalize_columns(dmatrix)\n",
    "show_rounded_matrix(dmatrix_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.scatter(dmatrix_normed[0], dmatrix_normed[1])\n",
    "dmatrix_normed_transpose = np.transpose(dmatrix_normed)\n",
    "for i in range(len(dnames)):\n",
    "    pyplot.annotate(dnames[i], dmatrix_normed_transpose[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documentxdocument similarities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_doc_matrix = np.dot(dmatrix_normed.transpose(), dmatrix_normed)\n",
    "show_rounded_matrix(doc_doc_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_heatmap(doc_doc_matrix, doc_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wordxword similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_reduced_normed = normalize_rows(T_reduced)\n",
    "term_term_matrix = np.dot(T_reduced_normed, T_reduced_normed.transpose())\n",
    "show_rounded_matrix(term_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_heatmap(term_term_matrix, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Folding in a search query\n",
    "\n",
    "To fold in a query, we find its vector just like any doc:**\n",
    "$\\large q' = T^Tq$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_folded = np.dot(T_reduced.transpose(), q_vec.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q_folded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a plot of the various documents and see where the query vector fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.scatter(Dt_reduced[0], Dt_reduced[1])\n",
    "D_reduced = np.transpose(Dt_reduced)\n",
    "for i in range(len(dnames)):\n",
    "    pyplot.annotate(dnames[i], D_reduced[i])\n",
    "pyplot.scatter(q_folded[0], q_folded[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_folded_norm = norm_vec(q_folded)\n",
    "pyplot.scatter(dmatrix_normed[0], dmatrix_normed[1])\n",
    "dmatrix_normed_transpose = np.transpose(dmatrix_normed)\n",
    "for i in range(len(dnames)):\n",
    "    pyplot.annotate(dnames[i], dmatrix_normed_transpose[i])\n",
    "pyplot.scatter(q_folded_norm[0], q_folded_norm[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = list(np.dot(q_folded_norm, dmatrix_normed))\n",
    "for i in range(len(cosines)):\n",
    "    print(dnames[i] + \": \" + str(round(matches[i], 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This is what they were without LSI**\n",
    "\n",
    "* c1: 0.816\n",
    "* c2: 0.0\n",
    "* c3: 0.354\n",
    "* c4: 0.289\n",
    "* c5: 0.0\n",
    "* m1: 0.0\n",
    "* m2: 0.0\n",
    "* m3: 0.0\n",
    "* m4: 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, about the use of a training corpus\n",
    "\n",
    "In the above procedure, we first found vectors for words in our new space. Then we used those word vectors to get the vector for any document. To do this we used the same documents that we wished to examine in order to find the word vectors.\n",
    "\n",
    "An alternative is to use a *training corpus* to find the word vectors. \n",
    "\n",
    "Suppose that we have a relatively small number of short student responses about global warming, and we wanted to compare those responses to each other or to model answers. We could go to the internet and build a training corpus consisting of hundreds or thousands of documents about global warming. Then we could \"learn\" the word vectors from that training corpus. In essence, this would allow us to learn the similarities between words, which we could use to help us with the student essays.\n",
    "\n",
    "So, for example, using the training corpus, we could discover word vectors for \"heat\" and \"warmth\" that were similar. Then, when creating vectors for the student responses, heat and warmth would make similar contributions to that tvector."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
