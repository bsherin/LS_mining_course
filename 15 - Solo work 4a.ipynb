{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solo Work 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A bit more about pizza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here, I'd like you to do a little bit more on the pizza dataset:**\n",
    "\n",
    "* First: Try using slightly different parts of the corpus for training and testing. Do the results vary?\n",
    "* Second: See if you can make a better classifier than we did in class. \n",
    "    * One thing to try would be to experiment with the vocabulary. You can try a smaller or larger vocabulary.\n",
    "    * You could try tailoring the vocabulary, perhaps by messing with the stop list.\n",
    "    * You could also try adding other features entirely to the feature set. We looked at the length of posts in class. You can play with that. You could also maybe look for phrases.\n",
    "    * You could try some of the other classification algorithms provided by NLTK? Maybe you could get decision trees to work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I copied over some big parts of notebook 14 to save you some trouble**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "csvfile = open('corpora/pizza.csv')\n",
    "pizza_reader = csv.reader(csvfile)\n",
    "data_list = []\n",
    "for row in pizza_reader:\n",
    "    data_list.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bruces_twitter_tokenizer(text):\n",
    "    import re\n",
    "    \n",
    "    def is_contraction(the_text):\n",
    "        contraction_patterns = re.compile(r\"(?i)(.)('ll|'re|'ve|n't|'s|'m|'d)\\b\")\n",
    "        return contraction_patterns.search(the_text)\n",
    "    \n",
    "    punctuation_class = r\"([\\.\\-\\/&\\\";:\\(\\)\\?\\!\\]\\[\\{\\}\\*#])\"\n",
    "    \n",
    "    # eliminate_urls\n",
    "    text = re.sub(r\"http\\S*\", \"\", text)\n",
    "    \n",
    "    # elimintate hashtags, user mentionds\n",
    "    text = re.sub(r\"#\\S*\", \"\", text)\n",
    "    text = re.sub(r\"@\\S*\", \"\", text)\n",
    "    \n",
    "    # Separate most punctuation at end of words\n",
    "\n",
    "    text = re.sub(r\"(\\w)\" + punctuation_class, r'\\1 \\2 ', text)\n",
    "    \n",
    "    # Separate most punctuation at start of words\n",
    "    text = re.sub(punctuation_class + r\"(\\w)\", r'\\1 \\2', text)\n",
    "    \n",
    "    # Separate punctuation from other punctuation\n",
    "    text = re.sub(punctuation_class + punctuation_class, r'\\1 \\2 ', text)\n",
    "    \n",
    "    # Put spaces between + and = signs and digits. Also %s that follow a digit, $s that come before a digit\n",
    "    text = re.sub(r\"(\\d)([+=%])\", r'\\1 \\2 ', text)\n",
    "    text = re.sub(r\"([\\$+=])(\\d)\", r'\\1 \\2', text)\n",
    "    \n",
    "    # Separate commas if they're followed by space.\n",
    "    # (E.g., don't separate 2,500)\n",
    "    text = re.sub(r\"(,\\s)\", r' \\1', text)\n",
    "    \n",
    "    #when we have two double quotes make it 1.\n",
    "    #\n",
    "    text = re.sub(\"\\\"\\\"\", \"\\\"\", text)\n",
    "\n",
    "    # Separate leading and trailing single and double quotes .\n",
    "    text = re.sub(r\"(\\'\\s)\", r' \\1', text)\n",
    "    text = re.sub(r\"(\\s\\')\", r'\\1 ', text)\n",
    "    text = re.sub(r\"(\\\"\\s)\", r' \\1', text)\n",
    "    text = re.sub(r\"(\\s\\\")\", r'\\1 ', text)\n",
    "    text = re.sub(r\"(^\\\")\", r'\\1 ', text)\n",
    "    text = re.sub(r\"(^\\')\", r'\\1 ', text)\n",
    "    text = re.sub(r\"('\\'$)\", r' \\1', text)\n",
    "    text = re.sub(r\"('\\\"$)\", r' \\1', text)\n",
    "\n",
    "    #Separate parentheses where appropriate\n",
    "    text = re.sub(r\"(\\)\\s)\", r' \\1', text)\n",
    "    text = re.sub(r\"(\\s\\()\", r'\\1 ', text)\n",
    "\n",
    "    # Separate periods that come before newline or end of string.\n",
    "    text = re.sub('\\. *(\\n|$)', ' . ', text)\n",
    "    \n",
    "    # separate single quotes in the middle of words\n",
    "    # text = re.sub(r\"(\\w)(\\')(\\w)\", r'\\1 \\2 \\3', text)\n",
    "    \n",
    "    # separate out 's at the end of words\n",
    "    text = re.sub(r\"(\\w)(\\'s)(\\s)\", r\"\\1 s \", text)\n",
    "    split_text = text.split()\n",
    "    \n",
    "    return split_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data_list = []\n",
    "for row in data_list:\n",
    "    new_row = [bruces_twitter_tokenizer(row[0].lower()), row[1]]\n",
    "    tokenized_data_list.append(new_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting each text item to a featureset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"lists/stop-words_english_5_en.txt\")\n",
    "stop_list = f.read().split(\"\\n\")\n",
    "stop_list += list('!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~â€™')\n",
    "stop_list += list(\"abcdefghijklmnopqrstuvwxyz0123456789\")\n",
    "stop_list = set(stop_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create a frequency distribution, ignoring words on the stop list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "word_fdist = nltk.FreqDist() # the corpus frequences\n",
    "for row in tokenized_data_list:\n",
    "    for word in row[0]:\n",
    "        if not word in stop_list:\n",
    "            word_fdist[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our vocabulary will be the 100 most common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = word_fdist.most_common(100)\n",
    "vocab = [w[0] for w in mc]\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_feature_extractor(the_text, vocab):\n",
    "    features = {}\n",
    "    for word in vocab:\n",
    "        features['contains(%s)' % word] = (word in the_text)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_feature_extractor(tokenized_data_list[0][0], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_featuresets = []\n",
    "for row in tokenized_data_list:\n",
    "    new_lf = [word_feature_extractor(row[0], vocab), row[1]]\n",
    "    labeled_featuresets.append(new_lf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate all of the labeled featuresets into a test set and a training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm going to reserve just about 10% of the data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = labeled_featuresets[:400]\n",
    "train_set = labeled_featuresets[400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza_classifier = nltk.NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.classify.accuracy(pizza_classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_list = [t[1] for t in test_set]\n",
    "test_list = [pizza_classifier.classify(t[0]) for t in test_set]\n",
    "cm = nltk.ConfusionMatrix(gold_list, test_list)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
